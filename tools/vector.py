import streamlit as st
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from graph import graph  # ä½ çš„ Neo4j é€£ç·šè¨­å®š

from langchain_community.vectorstores import Neo4jVector
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate

# âœ… 1. å»ºç«‹ OpenAI å‘é‡æ¨¡å‹
embedding = OpenAIEmbeddings(model="text-embedding-3-small")  # æˆ– "text-embedding-ada-002"

# âœ… 2. å»ºç«‹ Neo4j Vector Index
neo4jvector = Neo4jVector.from_existing_index(
    embedding=embedding,
    graph=graph,
    index_name="entity_vector",
    node_label="PharmConcept",
    text_node_property="description",
    embedding_node_property="descriptionEmbedding",
    retrieval_query="""
RETURN
    node.description AS text,
    score,
    {
        name: node.name,
        type: node.type,
        relatedGenes: [(g:Gene)-[:RELATED_TO]->(node) | g.name],
        relatedDrugs: [(d:Drug)-[:AFFECTS]->(node) | d.name],
        relatedConditions: [(c:MedicalCondition)-[:TREATED_BY|ASSOCIATED_WITH]->(node) | c.name],
        guidelines: [(g:Guideline)-[:RECOMMENDS]->(node) | g.name],
        source: node.source,
        source_url: 'https://yourdomain.com/source/' + toString(node.source)
    } AS metadata
"""
)

# âœ… 3. å»ºç«‹ Chat æ¨¡å‹
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# âœ… 4. å»ºç«‹ QA Chain
retriever = neo4jvector.as_retriever()

instructions = (
    "You are a pharmacogenomics assistant. Use the provided context to answer the question. "
    "If the answer is not in the context, say 'I don't know'. "
    "Context: {context}"
)

prompt = ChatPromptTemplate.from_messages([
    ("system", instructions),
    ("human", "{input}")
])

question_answer_chain = create_stuff_documents_chain(llm, prompt)
pharm_retriever = create_retrieval_chain(retriever, question_answer_chain)

# âœ… 5. å›ç­”æŸ¥è©¢å‡½å¼
def get_pharmacogenomics_answer(input):
    return pharm_retriever.invoke({"input": input})

# # âœ… 6. Streamlit ä»‹é¢
# st.title("ğŸ’Š Pharmacogenomics Assistant")

# if "messages" not in st.session_state:
#     st.session_state.messages = []

# for msg in st.session_state.messages:
#     with st.chat_message(msg["role"]):
#         st.markdown(msg["content"])

# if prompt := st.chat_input("è«‹è¼¸å…¥ä½ çš„å•é¡Œ..."):
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(prompt)

#     with st.chat_message("assistant"):
#         try:
#             response = get_pharmacogenomics_answer(prompt)
#             answer = response.get("answer", "I'm not sure.")
#             st.markdown(answer)

#             # âœ… é¡¯ç¤ºå¼•ç”¨ä¾†æº
#             docs = response.get("context", [])
#             if docs:
#                 st.markdown("---")
#                 st.markdown("**References:**")
#                 for i, doc in enumerate(docs):
#                     meta = doc.metadata
#                     name = meta.get("name", f"Source {i+1}")
#                     url = meta.get("source_url")
#                     if url:
#                         st.markdown(f"- [{name}]({url})", unsafe_allow_html=True)
#                     else:
#                         st.markdown(f"- {name}")

#             st.session_state.messages.append({"role": "assistant", "content": answer})

#         except Exception as e:
#             st.error(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
